{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kompi/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py:2349: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  output = genfromtxt(fname, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from nilearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "abide = datasets.fetch_abide_pcp(data_dir=\"./project/data/nilearn\",\n",
    "                                 pipeline=\"cpac\",\n",
    "                                 quality_checked=True)\n",
    "\n",
    "abide_pheno = pd.DataFrame(abide.phenotypic)\n",
    "\n",
    "groups = []\n",
    "for s in abide_pheno.SITE_ID:\n",
    "    groups.append(s.decode()) # for some reason the site names are of type 'bytes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_data(abide, output_dir, pipeline = \"cpac\", quality_checked = True):\n",
    "    # get dataset\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    # make list of filenames\n",
    "    fmri_filenames = abide.func_preproc\n",
    "\n",
    "    # load atlas\n",
    "    multiscale = datasets.fetch_atlas_basc_multiscale_2015()\n",
    "    atlas_filename = multiscale.scale064\n",
    "\n",
    "    # initialize masker object\n",
    "    masker = NiftiLabelsMasker(labels_img=atlas_filename,\n",
    "                               standardize=True,\n",
    "                               memory='nilearn_cache',\n",
    "                               verbose=0)\n",
    "\n",
    "    correlation_measure = ConnectivityMeasure(kind='correlation', vectorize=True,\n",
    "                                             discard_diagonal=True)\n",
    "\n",
    "    try: \n",
    "        feat_file = os.path.join(output_dir, 'ABIDE_BASC064_features.npz')\n",
    "        X_features = np.load(feat_file)['a']\n",
    "        print(\"Feature file found.\")\n",
    "\n",
    "    except: # if not, extract features\n",
    "        X_features = [] # To contain upper half of matrix as 1d array\n",
    "        print(\"No feature file found. Extracting features...\")\n",
    "\n",
    "        for i,sub in enumerate(fmri_filenames):\n",
    "            # extract the timeseries from the ROIs in the atlas\n",
    "            time_series = masker.fit_transform(sub)\n",
    "            # create a region x region correlation matrix\n",
    "            correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "            # add to our container\n",
    "            X_features.append(correlation_matrix)\n",
    "            # keep track of status\n",
    "            print('finished extracting %s of %s'%(i+1,len(fmri_filenames)))\n",
    "        # Save features\n",
    "        np.savez_compressed(os.path.join(output_dir, 'ABIDE_BASC064_features'),\n",
    "                                         a = X_features)\n",
    "\n",
    "    print(\"Running PCA...\")\n",
    "    pca = PCA(0.99).fit(X_features) \n",
    "    X_features_pca = pca.transform(X_features)\n",
    "\n",
    "\n",
    "    # Transform phenotypic data into dataframe\n",
    "    abide_pheno = pd.DataFrame(abide.phenotypic)\n",
    "\n",
    "    # Get the target vector\n",
    "    y_target = abide_pheno['DX_GROUP'].to_numpy()\n",
    "    y_target = y_target - 1\n",
    "\n",
    "    return(X_features_pca, y_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Feature file found.\n",
      "Running PCA...\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_data(abide, \"./project/data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class abide_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = torch.from_numpy(np.expand_dims(data, axis=1)).type(torch.FloatTensor)\n",
    "        self.labels = torch.from_numpy(labels).type(torch.LongTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        return self.data[item], self.labels[item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer_1 = nn.Linear(571, 128) \n",
    "        self.layer_2 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, 2) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layer_1): Linear(in_features=571, out_features=128, bias=True)\n",
      "  (layer_2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t| Accuracy: 21.35\n",
      "\t| Accuracy: 21.84\n",
      "\t| Accuracy: 26.44\n",
      "\t| Accuracy: 28.57\n",
      "\t| Accuracy: 29.23\n",
      "\t| Accuracy: 29.23\n",
      "\t| Accuracy: 29.06\n",
      "\t| Accuracy: 29.23\n",
      "\t| Accuracy: 29.72\n",
      "\t| Accuracy: 29.72\n",
      "\t| Accuracy: 29.56\n",
      "\t| Accuracy: 29.56\n",
      "\t| Accuracy: 29.56\n",
      "\t| Accuracy: 29.39\n",
      "\t| Accuracy: 29.39\n",
      "\t| Accuracy: 29.39\n",
      "\t| Accuracy: 29.39\n",
      "\t| Accuracy: 29.39\n",
      "\t| Accuracy: 29.39\n",
      "\t| Accuracy: 29.23\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "log_interval = 100\n",
    "batch_size: int = 64\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_data = abide_dataset(X_train, y_train, transform=None)\n",
    "test_data = abide_dataset(X_test, y_test, transform=None)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            progress = int(100 * (i + 1) * train_loader.batch_size / len(train_loader.dataset))\n",
    "            print(f\"\\rEpoch {epoch + 1} | Progress: {progress}% | loss: {running_loss / log_interval:.3f}\", end=\"\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        correct = 0     \n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            correct += (outputs.squeeze().argmax(1) == labels).sum().item()\n",
    "\n",
    "    print(f'\\t| Accuracy: {100 * correct / len(X_train):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
